---
title: "Residuals Explanation"
author: "Brandon Ritchie"
date: "9/20/2021"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(grid)
library(latex2exp)
library(ggpubr)
```
# Big Picture
We have all been presented with two columns of quantitative data whether in Excel, R, Python, etc and made a scatter plot and fit a regression line to it. However, because of the simplicity of a couple of lines of code or selecting a button, it may be easy to overlook how that predicted line was created. By analyzing the concept of residuals we can begin to understand how this predicted line represents the predicted average y value per unit change of x.

## Residual

A residual is the y distance of a given observation or dot from the fitted line. The definition for a given point's residual is $r_i = Y_i - \hat{Y}_i$ with $Y_i$ representing the Y position of the data and $\hat{Y}_i$ representing the Y position of the fitted line.

Something to consider is that a negative residual, or a data point that is underneath the fitted line, represents an underestimation for a given data point. On the other hand, a positive residual represents an overestimation for the given data point.

```{r, message = FALSE, warning = FALSE}
## Creating Plots

arb_points <- data.frame(x = c(-5, -4.3, -4.1,-3, -2.5, -1, -1.1, -1.4, -0.6, 1, 1.4, 1.25),
           y = c(-5,-5.4, -4.6,-3.5,-2,-1,1.5,2.4,1.8,3.2,4.6,5.4))


p <- ggplot(arb_points, aes(x,y))+
  geom_line(data = data.frame(x2 = seq(-10,10), y2 = -0.2166667), aes(x = x2, y = y2), linetype = "dashed")+
  geom_point(size = 3)+
  geom_smooth(method = "lm", se = FALSE, color = "firebrick")+
  expand_limits(x = c(-6, 6),
                y = c(-6,6))+
  labs(x = "X", y = "Y")+
  geom_text(aes(label = TeX("$\\bar{Y}$", output = "character"), 9,-.7), parse=T, colour= "black", size = 5)+
  geom_text(aes(label = TeX("$\\hat{Y}$", output = "character"), -5.5,-6), parse=T, colour= "black", size = 5)+
  geom_text(aes(label = TeX("$Y_i$", output = "character"), -0.3,-1.6), parse=T, colour= "black", size = 5)+
  theme_bw()

# Residual
residual <- p+
  geom_segment(aes(x = -1, y = 0.8, xend = -1, yend= -1), size = 1.5, color = "grey")+ 
  geom_text(aes(label=TeX("$r_i = Y_i - \\hat{Y}_i$", output = "character"), 0.4,0.3), parse=T, colour= "black", size = 5)+
  labs(title = "Single Residual Example")

# SSE
SSE <- p+
  geom_rect(xmin = -1, xmax = 0.8, ymin = -1, ymax = 0.8, fill = "grey")+ 
  geom_text(aes(label=TeX("$(Y_i - \\hat{Y}_i)^2$", output = "character"), 2.5,.5), parse=T, colour= "black", size = 5)+
  labs(title = "Single Squared Residual Example")

# SSR
SSR <- p+
  geom_line(data = data.frame(x2 = seq(-10,10), y2 = -0.2166667), aes(x = x2, y = y2), linetype = "dashed")+
  geom_rect(xmin = -1, xmax = .1, ymin = -.21, ymax = 0.8, fill = "grey")+
  geom_point(aes(x = -1, y = 0.8), size = 3, color = "red")+
  geom_curve(aes(x = 1, y = 0.5, xend = -0.5, yend = 0.5, curvature = 1), arrow = arrow(length = unit(0.03, "npc")))+
  geom_text(aes(label=TeX("$(\\hat{Y}_i - \\bar{Y})^2$", output = "character"), 2.2,.5), parse=T, colour= "black", size = 5)+
  labs(title = "Single Squared Regression Error Example")

# SSTO
SSTO <- p+
  geom_rect(xmin = -1, xmax = -.1, ymin = -1, ymax = -0.2166667, fill = "grey")+
  geom_curve(aes(x = 1, y = -0.5, xend = -0.5, yend = -0.5, curvature = 1), arrow = arrow(length = unit(0.03, "npc")))+
  geom_text(aes(label=TeX("$(\\Y_i - \\bar{Y})^2$", output = "character"), 2.2,-.5), parse=T, colour= "black", size = 5)+
  labs(title = "SSTO for a single point example")
```

```{r, message = FALSE, warning = FALSE}
residual
```

In this example, our $Y_i$ data point is an underestimate of the predicted average for an item that has an x value of 0. If our predicted line matches that of the true line and we sampled several points with an explanatory value of 0, then the mean of the values should be on the line (y = 2.7).

But how is this predicted line created based on these points? An inuitive response may be to take the absolute value of all residuals and find a line that reduces the mean value of these residuals. We will explain deeper in the following section.

## SSE (Sum of squared error) and MSE (Mean squared error)

What if instead of looking at the residuals in a one dimensional space of distance we look at the distance in the form of an area? This is accomplished by squaring our residual value. We know from geometry that the area of a square is found by squaring one of it's sides.

The following image shows the squared error for our point we analyzed above.

```{r, message = FALSE, warning = FALSE}
SSE
```

If we take each point, find it's squared error, and sum them up we get $SSE$ or sum of squared error. The formula for what we described is $SSE = \sum_{i = 1}^{n}(Y_i - \hat{Y}_i)^2$. The purpose of taking the sum of the squared residuals is two-fold. First, in a squared form it makes it easier to take the derivative which, when set to zero, gives the intercept and slope that will minimize the sum of squared errors. The second reason for squaring the values is that an area penalizes outliers more and favors several points with a small residual. For example if you have a residual of a point that is $r_1 = 3$ and a point that is $r_2 = -10$, if you were to sum their absolute values you would get a sum of 13. However, if you square them you will get $r_1^2 = 9$ and $r_2^2 = 100$. You can see that by squaring these values the order of magnitude for $r_2$ relative to $r_1$ doubles. In short, the sum of squared errors can be described as the total area that the data departs from the fitted regression line $\hat{Y}$. By dividing this value by the number of data points minus the number of parameters we can get the average area that a given point departs from the fitted line. This is called the mean squared error and it is defined as: $$MSE = \frac{\sum_{i = 1}^{n}(Y_i - \hat{Y}_i)^2}{n - p} = \frac{SSE}{n - p}$$

## SSR (Sum of Squared Regression Error)

The SSR is defined by the following equation: $SSR = \sum_{i = 1}^{n}(\hat{Y}_i - \bar{Y})^2$. If we were to visualize this for a single data point the graph would look as follows:

```{r}
SSR
```

The red dot in the regression represents the predicted Y value at the point $Y_i$. $\bar{Y}$ represents the average y value for the data. Something to note is that as the slope of the regression increases, the SSR also increases. However the total value for the SSR by itself is relatively arbitrary and it is an intermediate step to determine the fit of the model as a whole.

## SSTO (Total Sum of Squares)

With an understanding of what SSE and SSR are and how they are calculated we can start to understand SSTO or total sum of squares. The total sum of squares is defined as $SSR + SSE = SSTO = \sum_{i = 1}^{n}(Y_i - \hat{Y}_i)^2$. We can see that is we add up all of the squared residual values and squared regression error values we are left with the total sum of squares. However, what is more interesting is that if you sum the square difference from $Y_i$ to $\bar{Y}$ of all of the squares, you get the same value as if you summed all of the squared residuals and squared regression errors. We can visualize that for one point below.
```{r}
SSTO
```

When first looking at this graph you may expect the box to be larger. After all, the SSTO = SSR + SSE so shouldn't the area of this box represent the total area of that points squared residual and squared regression error. Not quite. Remember that the SSR and SSE are the sums of the box's at all of the points. In other words, at any given point $i$, $(Y_i - \hat{Y}_i)^2 + (\hat{Y}_i - \bar{Y})^2 \neq (Y_i - \bar{Y}_i)^2$ but $\sum_{i = 1}^{n}(Y_i - \hat{Y}_i)^2 + \sum_{i = 1}^{n}(\hat{Y}_i - \bar{Y})^2 = \sum_{i = 1}^{n}(Y_i - \bar{Y}_i)^2$!

## $R^2$ (R-squared)

Now that we have a foundation in the different square functions, we can tie it all together with how these functions can be used to determine the overall variability of the data. If we collapsed all of our data onto the y-axis, and we can see the distribution overall variability of the data. By adding a second dimension of X we can now view the data in a two-dimensional plane. The question now is, how much of the variability in the data can be described by this variable X and how much is due to unknown factors. R squared is the measure of variability explained by the independent variable X. It is defined as:
$$R^2 = 1 - \frac{SSE}{SSTO} = \frac{SSR}{SSTO}$$

If we isolate these values we can understand better what is going on. If we refresh, SSR represents the sum of the squared regression errors from the mean and SSE represents the sum of the squared residuals. If we take a single data point and translate it incrementally across the y-axis we can ask ourselves what happens to the SSE and SSR as the data moves closer to the line or away from the line assuming the fitted line stays fixed? You may realize that the squared regression error for the point does not change on a fixed line, but as the data point moves closer to the line, the squared residual value decreases. If we look at this in terms of the data as a whole, as the data points move closer to the fitted regression line the SSE value decreases. We know since $SSTO = SSE + SSR$ that as the SSE value becomes smaller the SSR value begins to comprise a larger proportion of SSTO. Because SSR can never be greater than SSTO, as SSE becomes smaller the $R^2$ value approaches 1. As the SSE value becomes larger, the regression line has less slope and the SSR decreases, and as it approaches 0 the $R^2$ value approaches 0. Thus $R^2$ is a measure of variability explained by an independent variable between the values 0-1, with 1 being all of the observed variation being explained by the independent variable and 0 being none of the observed variation being explained by the independent variable.

Because of the complexity of visualizing R-squared on a graph I am going to create a visual representation of a graph with a small and large $R^2$ value. If the boxes don't look like squares it is due to the window output.
```{r}
## Re-reading in Weather Dat
# Reading in and Wrangling Data
dat <- read_csv("C:/Users/brand/Documents/Statistics-Notebook/Data/rexburgWeather.csv") %>% 
  select(-c(Wind))

# Creating a key table to shift relationship Days back 3 Days
key_table <- dat %>% 
  count(Year, Day) %>% 
  mutate(twokey = paste0(as.character(Day - 2), "(", Year, ")"),
         key = paste0(Day, "(",Year,")")) %>% select(-n)

dat2 <- dat  %>% 
  rename(Pressure = Barometer) %>% 
  mutate(groupFactor = paste0(Day, "(",Year,")")) %>% 
  group_by(groupFactor) %>% 
  summarise(HighTemp = max(Temp),
            AvgTemp = mean(Temp),
            AvgPressure = mean(Pressure),
            AvgHumidity = mean(Humidity)) %>% 
  ungroup() %>% 
  rename(key = groupFactor) %>% 
  inner_join(key_table, by = "key")
# Columns with prefix Two are values two Days prior

threeAve <- dat2 %>% 
  filter(Day != 20) %>% 
  group_by(Year) %>% 
  summarise(threeAveHigh = mean(HighTemp),
            threeAveTemp = mean(AvgTemp),
            threeAvePress = mean(AvgPressure),
            threeAveHum = mean(AvgHumidity))

fin_dat <- dat2 %>% 
  filter(Day == 20) %>% 
  rename(HighTempTarget = HighTemp) %>% 
  select(HighTempTarget, Year) %>% 
  inner_join(threeAve, by = "Year")
# Rescaling Values and Performing Regression on each Variable
fin_dat2<- fin_dat %>% 
  mutate(AveHighRescale = ((threeAveHigh - min(threeAveHigh))/(max(threeAveHigh) - min(threeAveHigh)))*10,
         AveTempRescale = ((threeAveTemp - min(threeAveTemp))/(max(threeAveTemp) - min(threeAveTemp)))*10,
         AvePressRescale = ((threeAvePress - min(threeAvePress))/(max(threeAvePress) - min(threeAvePress)))*10,
         AveHumRescale = ((threeAveHum - min(threeAveHum))/(max(threeAveHum) - min(threeAveHum)))*10) %>% 
  select(-c(threeAveHigh, threeAveTemp, threeAvePress, threeAveHum))

# Find Correlations
HighLM <- lm(HighTempTarget ~ AveHighRescale, data = fin_dat2)
summaryHigh <- summary(HighLM)

TempLM <- lm(HighTempTarget ~ AveTempRescale, data = fin_dat2)
summaryTemp <- summary(TempLM)

PressLM <- lm(HighTempTarget ~ AvePressRescale, data = fin_dat2)
summaryPress <- summary(PressLM)

HumLM <- lm(HighTempTarget ~ AveHumRescale, data = fin_dat2)
summaryHum <- summary(HumLM)
# Creating Predictor Variable
regression_data <- fin_dat2 %>% 
  mutate(predictor = (((1/summary(HighLM)$coefficients[,4][2])*AveHighRescale) + ((1/summary(TempLM)$coefficients[,4][2])*AveTempRescale)+((1/summary(PressLM)$coefficients[,4][2])*AvePressRescale)+((1/summary(HumLM)$coefficients[,4][2])*AveHumRescale))/4) %>% 
  select(HighTempTarget, predictor)

mainLM <- lm(HighTempTarget ~ predictor, data = regression_data)

plot <- regression_data %>% 
  ggplot(aes(x = predictor, y = HighTempTarget))+
  geom_point()+
  geom_smooth(method = lm, se = FALSE, color = "firebrick")+
  theme_bw()

R2_1 <- ggplot()+
  geom_rect(aes(xmin = 18, xmax = 33, ymin = 35, ymax = 50), fill = "grey")+
  geom_rect(aes(xmin = 10, xmax = 40, ymin = 0, ymax = 30), fill = "grey")+ 
  scale_x_continuous(limits = c(0, 50))+
  geom_text(aes(label = "SSR = 209.485", 25,42))+
  geom_text(aes(label = "SSTO = 844.019", 25,15))+
  geom_text(aes(label = TeX("$R^2 = 0.248$", output = "character"), 45,40), parse=T, colour= "black", size = 5)+
  theme_bw()

ggarrange(plot, R2_1)
```

This first graph is a prediction of the high temperature of weather based on a predictor variable. As we can see the SSR is 209 and the SSTO is 844 therefore we get a low $R^2$ value of 0.248.

```{r}
p2 <- ggplot(arb_points, aes(x,y))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE, color = "firebrick")+
  theme_bw()

r2_2 <- ggplot()+
  geom_rect(aes(xmin = 19, xmax = 31, ymin = 14, ymax = 26), fill = "grey")+
  geom_rect(aes(xmin = 18, xmax = 32, ymin = 0, ymax = 13), fill = "grey")+ 
  scale_x_continuous(limits = c(0, 50))+
  geom_text(aes(label = "SSR = 150.53", 25,20))+
  geom_text(aes(label = "SSTO = 163.71", 25,7))+
  geom_text(aes(label = TeX("$R^2 = 0.919$", output = "character"), 45,25), parse=T, colour= "black", size = 5)+
  theme_bw()

ggarrange(p2, r2_2)
```

For the data we have been modeling the SSR and SSTO are much closer to the same value, therefore we have a higher $R^2$ of 0.919.

## How does R Squared differ from the p-value?

As we discussed in the previous section, as your $R^2$ decreases that means that a the data is explained less by the x value. As a result the slope of the line decreases. As the slope decreases the p-value for the slope increases and becomes less significant. We should remember however that p-value is the measure of the probability of obtaining a test statistic as extreme as the one observed which differs from the definition of R squared.

## How does R Squared relate to the residual standard error?

The residual standard error is defined as:

$$RSE = \sqrt{\frac{(Y_i - \hat(Y)_i)}{n - p}}$$

This can be compared to the standard deviation. You may recognize that the numerator is the SSE. We can re-write the formula as $SSE = (n-p)(RSE)^2$.
If we plug this into our R squared formula it looks like $$R^2 = 1 - \frac{(n-p)(RSE)^2}{(n-p)(RSE)^2 + SSR}$$


